# Using Infrastructure as Code tools to define ownership and responsibility - Transcript

DAWN: I'm guessing that you can all hear me?

ROB: I can!

DAWN: Excellent!  I regret to inform you that despite my constant protestations to the universe, I have not yet managed to grow a third hand.

ROB: [laughs]

DAWN: Let me just start everything up - I just need to give it - Now, if we plug this in, one hopes that at some stage it will - 

ROB: Fingers crossed.

DAWN: - magically work.  Although, considering the earlier technical difficulties, that may be opti... mistic!  There we go.

ROB: Yay!

DAWN: All right.  [pause]  Well.  Ah, for the second time - actually, for the third time in three months, because I believe I did What's New in AWS last month - erm, I am here again at the  Melbourne AWS User Group and it is lovely to be here.  Today I'm going off my, sort of, standard beaten path of the governance, risk, and compliance services, ah, and other things along those lines that I generally come to rant to you about.  And we are going to talk about using infrastructure as code tools to define ownership and responsibility across your organisation.

To begin with, I would like to acknowledge the Wundj - the Wurundjeri people of the Kulin Nation - it would help, apologies to them, if I could actually pronounce that name properly - on whose land I am presenting tonight.  This is, and always will be, Aboriginal land.  Their sovereignty was never ceded.

I'm hoping that considering that this is my third time here in three months, most of the audience knows who I am.  But, for those of you who have not met me before; my name is Dawn; I do cloud security and DevOps type work at Innablr - and we are hiring, so I'm sure you'll be hearing from me again in the marketplace segment; erm, what probably pretty much no one here knows about me though, is that I am the child of two PhD holders.  It's probably not immediately apparent what being the child of two PhD holders has to do with defining infrastructure as code across your organisation.  Erm, for that I would say to you all that being the child of two PhD holders, and having read their PhDs, teaches you the value in repeatable experiments, and in being able to validate your findings in multiple scenarios.  Outside of work, I am an occasional author and kitchen alchemist - sometimes that ends really well, sometimes it doesn't.  I'm also a raging sportsball fan, and I am taking a break from the hockey garb which has been common recently, and we're going with New England Patriots football gear instead today.

PATRIOTS FAN IN AUDIENCE: [claps]

DAWN: So, back to that thing about being the child of two PhDs.  The reason why it is relevant for this talk is because, the- although what I'm talking about is infrastructure as code tools, this is predicated on a concept called Conway's law, which, as the name would suggest, was invented by a gentleman called Conway.  A paraphrase of what this is about is to say that 'Conway's law is- the maxim that an organisation will always design - tools, software, products, whatever - which reflect the organisation's structure'.  And that means that by first principles, if you want to design yourself a system, you have to start by designing the structures that are going to build it.  You have to design the people which are going to build it.  You have to design an architecture of teams; an architecture of reporting which will enable you to actually build what you want to build.  And as befits my parents and their PhDs, Conway's law has actually been validated in multiple academic settings by researchers who have gone and looked at big companies such as Microsoft.  And they found that generally this maxim holds true.

So why is it that org structure matters? This is signifi- or, my take on this, at least, is significantly less researched than the fact that it does, but the first thing that I would call out here is that if teams actually talk to each other, they're going to work much faster, and the work that they're going to produce is generally going to be a lot better.  When you have people sitting alone in silos developing things, there's often a lot of information in people's heads which is not cross-pollinating.  And if that information were cross-pollinating, it would potentially speed up the work that you were trying to do.  A colleague of mine at Innablr called Colin Liddle - really great guy - wrote a blog post for us a while back talking about the idea of platform teams.  Colin and I also worked together at a previous job, and both of us were, at various times, part of the platform team there.  And one takeaway that he called out in that blog post which I absolutely agree with is that the paradigm of the platform team, which is one which is- sort of becoming much more common now - it means that if you have people who are maintaining your baseline, you free up the dev teams to get work done.

If every team is managing their own Kubernetes cluster, and I know that Kubernetes is becoming a bit of a schtick here, but that means that 95% of their time is going to be taken up actually managing the infra to make sure that it's working.  Even if it is Kubernetes, if you can centralise that all in one place and have a group of specialists managing it, then all that the people who are deploying to it need to know is about what they need to do to deploy.  And those dedicated support people are inherently going to reduce blockers.  They are going to reduce lead times.  Because when you have a problem that's not in your domain, if you've got dedicated support people who can help you with it, you're not smashing your head repeatedly against a brick wall.  You can go to them, interact with them, and that will enable you to get things done quicker.  And ultimately this idea of, sort of, cross-functional working groups ends up being a bit fallacious a lot of the time, because - they're not consistent, and they're often arranged on an ad-hoc basis.  If you have a simple and appropriate team structure, which kind of matches the size and the scale of your organisation, in the end, what you're going to end up with is a simpler final product.  For large organisations, some degree of complexity is inherent to their structure, but if you can work out how to harness that complexity in the correct way, it will allow you to produce software which is ultimately much more simple.

So, this brings me to the question of how to architect a company - but, as those of you who've seen all of my talks would know, it's probably better described as how not to architect a company.  Because a lot of the companies that are around - even big companies, even companies like AWS - once upon a time were a bunch of people in a shed, or a bunch of people working for another company, who eventually got spun off into their own structure.  And if you do not think about solution architecture and design architecture for your organisation, what you end up doing is building complexity in on the fly.  You scale, but you don't necessarily scale in a way that's going to keep you running at the same velocity when you're ten times or a hundred times bigger.  And what a lot of that complexity tends to come in the form of i- are siloed teams that don't have a central support structure.  You need to scale, you spin up another team to run it.  But then you've got that problem of a whole bunch of redundant work is being done.  There's information in multiple people's heads.  Everyone has various different pieces of the jigsaw, but you can't put together the puzzle until you get all of them together, so they have to go and find the bits themselves.

The other fallacy, which I think is slightly less common but I'm aware of a few examples of this in the wild, is when you get really deep team-based hierarchies.  So rather than one or two levels, you've got four or five.  You've got different levels of teams that are building complexity on top of each other.  And what will tend to happen there is you lose critical information in communication.  If you've got one people that are buildi- one group of people that are building the infrastructure, and then you've got another group of people that are building on top of that, and a third group that's building on top of that, and then a fourth group that's finally building your application, how does the information that the application team need from the people who are building that core infrastructure get through those layers of hierarchy?

And again, as befits my talks at the AWS User Group, once again, we are going to return to my favourite case study.  And that is our friends at ProductCorp and their newly acquired subsidiary Fly Money.  I'm not going to do the full introduction, because I'm sure that most of you have met ProductCorp and Fly Money by now, but we're talking here about two startups.  ProductCorp builds primarily e-commerce software, and they have two flagship products; BuyIt and SellIt.  They run everything on AWS.  And the reason why they bought Fly Money is because they were paying a significant amount of money to a payments processor to handle all of the payments on their purchasing software, and they decided that they needed to cut out the middleman.  And, we have- anyone who has seen my ProductCorp talks before will also have met the wonderful Ben Nguyen.  They are ProductCorp's Head of Engineering, and they have over 10 years of experience, mainly in the venue of startups.  To be honest, given how long I've been giving these talks, it's probably up near 15 now.  But one thing that's really important to understand about Ben and the way that they approach things is that they're very passionate about improving infrastructure.  And they're very well aware that part of what comes out of that is - it's not just about improving infrastructure, you have to improve the structures that people are working in to enable them to build better things.

Which leads us to- taking a few steps back, as we've sort of seen this acquisition unfold.  Let me tell you a story about the first meeting of minds.  This is when Ben got all of the employees of Pr- of Fly Money's engineering department into a room, and they had a clash with meetings.  So they got everyone in the room and they said 'What I want you to do, while I'm gone for these 15 minutes to take this critical call, is sit down and draw me out an org chart.  You know, I've heard about how your company operates, but what I really want to understand is from your point of view, what do you see as the structure of the organisation?  So get together, draw that out, and I'll be back in 15 minutes'. Ben came back 15 minutes later to not one org chart but one orc chart per person.  And the real kicker here, which shows you just how dysfunctional Fly Money is, is that there were absolutely no similarities between any of these 20-some org charts.  In fact, the one that turned out to be the most accurate was the one which just had the employee's name on it, and then drew out a chart with a bunch of question marks all over the place.  So as we can see from this, this is not really a company that has any sort of functioning structure.

And this brings me to the question of what Fly Money's initial infrastructure actually looked like.  Because the art of defining repeatable infrastructure is something that's - I mean, it's a fine art.  If you do it well it helps you to increase your velocity, but it's actually really, really difficult to do.  So the question there becomes who's actually responsible for things?  And this is my, ah, entirely anecdotal opinion on one particular model of responsibility that I have seen work very well when you're dealing with infrastructure as code.  So what you essentially have here is you have two groups of people.  You have- people who are in central teams, which are responshibl- responsible for each core component of the platform.  And that might be people who are responsible for AWS.  It might be people who are responsible for your ECS Fargate.  It might be people who are responsible for your Kubernetes, or, you know, maintaining things like your central security, but that might also be people who deal with other vendors.  People who are responsible for for provisioning for things like your Github or your Gitlab.  People who are responsible for provisioning other tools which are very important for you to be able to use, and keeping them up to date.  And then you have the dev teams, who are actually building the software on top of this.  They've got the core platform.  They don't need to be able to necessarily see under the hood of how all of that is working, all that they care about is that it works and that they have someone that they can go to to talk about it if they're running into any issues.

So your central teams are responsible for building and architecting those core components.  Getting everyone in a room, deciding how it should run, and then setting it up.  They're also responsible for providing some sort of pattern that dev teams can consume to allow them to use the software.  And if you're talking about a Git provider that's probably going to be very simple, but if you're talking about some sort of platform built on AWS, whether that be just a basic account structure or whether it be that you're actually providing a lot of the building blocks to people, it's going to need to be significantly more complex.  You can't rely on dev teams being able to understand CloudFormation defined in YAML.  And finally, they're responsible for meeting internal SLAs.  So they are responsible for being able to go to the dev teams and say 'we expect this to be up this amount of time, architect your software accordingly, and then this will inform the SLAs that you give to other businesses.  When you're talking to sales people, when you're talking to marketing people, these are the baselines you're working from, and then you build in your own slack'.

And the dev teams are responsible for taking those core patterns and customising them to their use-case.  It might be that they're basically usable out of the box, or it might be that there's a significant amount of customisation that needs to happen on top.  And they are also responsible for automated and reliable testing and deployment of their applications, and this is absolutely critical.  Although the core infra is only as good as the infrastructure as code and the processes that define it, the same is true for the application.  An application is only as good as the automation and reliability of the testing and deployment process.  You do not want to be picking up your major issues in manual UAT testing - even though manual UAT testing is still something that you should be doing.

And then we have a bunch of different tools which we can leverage to do all of this.  Infrastructure provisioning tools, which are the ones that we would generally actually use to set up stuff on AWS, fall into two buckets: you've got ones which are declarative, where you just write down what you want and it spits it out; and you've got ones which are programmatic, where you can set up functionality, feed it some information and it will generate it for you.  So we're obviously in AWS land right now, your main declarative tool is going to be CloudFormation.  If, for some reason, you're using Azure day to day and you're here at an AWS meet up - well, first off, kudos to you, but what you're looking for there are ARM templates.  And if, as is more likely, you're working in a multi-cloud environment or a hybrid cloud environment, the tools which are open-source to at least a degree which you can use in any scenario are going to be your Pulumis and your Terraforms.  There's always utility in being able to understand how these work, but it's also very important if you're working somewhere like a big bank or an emergency services provider where they need to actually run things on multiple clouds to provide a level of redundancy.  Whole bunch of fallacies around those sorts of things being multi-cloud or hybrid cloud - they're not, you cannot take the configuration from one of- one cloud and immediately plug it into another, but if you understand how to write Terraform for AWS, you understand how to write Terraform for Azure or GCP.  And programmatically, you've got our very own AWS CDK, which HashiCorp have built on to provide the CDK for Terraform.  These basically take really common programming languages: you define constructs, you tell them what you want, and they will generate the declarative template for you.  This is really, really powerful, although it comes with one or two major downsides.

The other big bucket that you've got here, which are the ones that are sort of falling out of favour, are the config management tools.  And there are four and a half of these that are very commonly used.  The four are Ansible, Chef, Puppet, and Salt.  And what these allow you to do is to basically define a list of changes that you want to make to your server, and then the config management tool will go away and make them.  The reason why these are slowly becoming obsolete is because - the most repeatable way to do config management is to actually build it into a container image so that you don't have to go away and manually do it on each server.

So how does this all apply to Fly Money?  Well, those of you who heard the disaster recovery presentation two months ago would know that they had some of their infrastructure as code partially defined in Terraform, but they've also got a whole bunch of it defined in CLI scripts.  So what you've essentially got there is - very similar to the config management tool - an imperative list of instructions that go 'hey, go away on my server and do this', rather than a declarative template that says 'for this particular structure, I want you to get it to this end state'.  What's worse is that because Fly Money's org chart is dysfunctional at best and non-existent at worst, about 60% of the work that's being done there is repeatable.  You don't need to invent the wheel on how to deploy an EC2 instance five times.  If you can find one pattern that works for deploying one, that will get you to where you want to be.  And their config management, contrary to the current common paradigm, is all done with Ansible and Chef.  But the real kicker here is the bulk of their resources, so about 55%, are not actually defined anywhere at all.  Someone has just gone in on the console, created these, and not thought to make them automatable.

So Ben's main question here is 'if I am going to wrangle Fly Money's organisational state to a functional state where everything is going to work, which tools do I want to use where and when to make sure that the separation of concerns is clearly defined?'  And there are a few considerations here around how you want to actually get your org to run.  The paradigm which Ben is going to use, and the one which I've presented to you tonight, is a very very simple one, where you have one level down the bottom of a platform team - or, some similarly named team that performs the same function of managing some degree of core infrastructure which people are going to share.  And they also manage provid- the- the idea of providing repeatable patterns to people, which they can go away and reuse.  There are other ways of architecting your organisation which work perfectly well, but this is a simple paradigm which scales very well and will generally work in most situations.

So in terms of the actual separation of concerns, you've got a few different levels at which you can do this.  At the very basic level for AWS, you're looking at your accounts and you're looking at your role-based access control.  On top of that, you're then going to build shared platforms.  And that might be your VPC and networking structure.  It might be your Fargate clusters, your ECS clusters.  It might be your Kubernetes clusters.  It might be that you're building in some level of shared platform that's based around something like CloudFront if a lot of what you're doing is related to content delivery, particularly if you're running static websites.  It might be that it's entirely serverless, and you've got some level of shared platform which you're building on something like API Gateway, where you expect everyone to consume one, and not each individual area of the company to set up their own.  And from those shared platforms, you want to build out reusable patterns, which people can then take, and can actually - in the dev teams, they can be responsible for managing and deploying their own infrastructure, and customising it to the specific use case that you're looking at, without them needing to know about everything that they need to do.  So then you've got the app infra on top of that, and on top of the app infra, those dev teams are going to need to worry to some degree about how they do config management.

So once Ben manages to wrangle Fly Money's set up into a reasonable state, what they end up with is a central platform team which is using CloudFormation for all of the resources that they deploy.  The advantage of this is that it's declarative, it sets out everything in one particular way; it is native to AWS, which is the only tool that ProductCorp and Fly Money use; and at the end of the day, it is also the one that has the most known bugs which are generally the least impactful.  Part of the reason why they chose not to use CDK for this infrastructure is that CDK doesn't actually have a native way of detecting drift.  You have to do that at the CloudFormation level once CDK has finished generating CloudFormation templates for you.  And when you're dealing with building shared infrastructure like VPCs, that can be quite bad.

So what you're looking for with the account level configuration, and that sort of shared layer, is you want to do it in the simplest manner possible.  And unless you're dealing with a bunch of people who understand code very well, it's highly likely that that way of doing it is going to be declarative.  You may also find that for what you want to build, something like CDK is actually more powerful for your account level config - in that case, go away and use that.  But where the tools like CDK, those functional infrastructure as code tools, really come into their own is at the next layer.  And those are the patterns that the platform team at ProductCorp/Fly Money is going to build out - the patterns which everyone across the organisation are going to be able to consume.  And that basically abstracts a whole lot of the work away from dev teams.  Rather than giving them massive declarative files which they then have to be able to understand and parse YAML to use, you give them very small reusable constructs with sensible defaults built in that you can entirely abstract away.  So th- the real power in a tool like CDK is in that particular area; patterns, reusable code, application infrastructure.  You build it once, you go and give it to 10 different teams, and 10 different teams can then build on top of it rather than having to go and architect it on their own.

When you actually finish building that infra, and once the dev teams have finished what they're under- you know, what they're- what they're building and they're now trying to deploy things, your next layer, which generally comes into play at the CI/CD point, becomes your config management.  And for every dev team at Fly Money, for every dev team in any org, what's critical here is not to go 'there is one perfect tool for this'.  What's critical is to recognise that config management methods all have different trade-offs.  There are scenarios where it is going to be ideal to use something like an Ansible, a Chef, or a Puppet; if you want to build on top of a very very basic container image or a bare operating system, and make changes in one scenario.  But if you're deploying that same code to - you know, twenty- is it 22 now? I think it's twenty- 22 with Jakarta - different AWS regions, there's a significant amount of time that goes into having to run that on each server, and you end up with a scenario where a lot of those configuration- a lot of those different environments, as you build on them, are going to drift from each other.  So that's a scenario where you'd want to be doing the vast majority if not all of your config in the actual consh- container image, and anything which is specific to your environment, rather than doing it using a config management tool, you call out for it.  So the decision there as a dev team, at the dev level, is basically 'decide which trade-offs you're willing to accept'.  Are you willing to accept that you might have to bake an image for three hours, but then you're deploying it out to 22 different regions, and that will take you five minutes?  Or do you really want to just run off a base image and do most of that configuration in one place, in one location, using one of those config management tools?

And that is the end of the presentation!  If you have any questions, I am happy to field them now.  Contacts, otherwise, are up on the slides, and if you want to explore the previous history of ProductCorp, there is also a link there to the Github repository that has all of the talks that I have done here before, ah, in video form, as well as sources for some of them, and transcripts!  Thank you.

ROB: Thank you Dawn!

AUDIENCE: [Applause]

ROB: Did anybody have any questions for Dawn?  [pause]  We are a quiet audience tonight, aren't we?

DAWN: [laughs, pause] I can see one?

AUDIENCE MEMBER 1: Those resources, the resources that Ben Nguyen has to deal with which were manually created in the console, what- what tools are available for him [sic] to pull those into his [sic] infrastructure as code platforms?

DAWN: It depends on what sort of resources they are, and it depends on what sort of infrastructure as code-

AUDIENCE MEMBER 1: IAM resources.  Lots of them.

DAWN: Yeah.  Erm, the one that I actually- the one that I know the answer to this best for is actually Terraform, because Terraform has an import functionality where you can basically give it - and depending on the resource, it'll be- an ARN, or an identifier, or an alias, you can basically go 'I have this Terraform stack, I have defined this Terraform stack locally, `terraform import` this resource'.  And Terraform will then go away, import it into the stack, and because Terraform does detect drift, it will then go and do change management against it.  I actually have not done this in CloudFormation for many, many years.  I know that there's a facility in CloudFormation to create a stack from existing resources, but for CloudFormation specifically, and for the CDK-type things, go away, research what the current method of doing it is.  I don't know for CDK.  I know for CloudFormation, you have that import functionality when you actually build the stack.  For Terraform it's easy, you can just do it on the fly.

AUDIENCE MEMBER 1: Thanks.

ROB: Are there any other questions?  Yep, just wait a second, bring the thing.  [pause] Get my steps up.

AUDIENCE MEMBER 2: There's a tool you can use called Cloud Former ([Former2](https://former2.com/)), I think?  Which can, ah, import.  Written by Ian in Sydney?  Ian Mckay?

DAWN: Yeah, that sounds about right.

AUDIENCE MEMBER 2: Erm, yeah, so that might be useful.

DAWN: Yeah.  Definitely, anything that Ian has written, I would absolutely endorse.  But yeah, that would also tend to suggest that the Cl- the functionality for CloudFormation natively is not quite there.  So if you're looking to - that's actually a- another fair point, that if you're looking to import a bunch of resources and you don't have that existing platform, maybe that's actually a compelling use case for Terraform if you've already got stuff.  Because Terraform will let you readily do those imports on the fly.  And that- that's part of the trade-off that you get with all of those sorts of things, I couldn't cover it all in 20 minutes but there are a lot of different scenarios which will lead you to one of these tools over the other, depending on what it is you're actually doing.

ROB: Were there any other questions? [pause] No? Thank you very much, Dawn.

AUDIENCE: [applause]

DAWN: Cool! I will- [pause, to Rob] I'm going to give that to you once I sit down, if that's okay.

[inaudible exchange between Dawn and Rob]

ROB: Cool! Thank you again, Dawn.

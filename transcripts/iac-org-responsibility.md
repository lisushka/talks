# Using Infrastructure as Code tools to define ownership and responsibility - Transcript

DAWN: I'm guessing that you can all hear me?

ROB: I can!

DAWN: Excellent!  I regret to inform you that despite my constant protestations to the universe, I have not yet managed to grow a third hand.

ROB: [laughs]

DAWN: Let me just start everything up - I just need to give it - Now, if we plug this in, one hopes that at some stage it will - 

ROB: Fingers crossed.

DAWN: - magically work.  Although, considering the earlier technical difficulties, that may be opti... mistic!  There we go.

ROB: Yay!

DAWN: All right.  [pause]  Well.  Ah, for the second time - actually, for the third time in three months, because I believe I did What's New in AWS last month - erm, I am here again at the  Melbourne AWS User Group and it is lovely to be here.  Today I'm going off my, sort of, standard beaten path of the governance, risk, and compliance services, ah, and other things along those lines that I generally come to rant to you about.  And we are going to talk about using infrastructure as code tools to define ownership and responsibility across your organisation.

To begin with, I would like to acknowledge the Wundj - the Wurundjeri people of the Kulin Nation - it would help, apologies to them, if I could actually pronounce that name properly - on whose land I am presenting tonight.  This is, and always will be, Aboriginal land.  Their sovereignty was never ceded.

I'm hoping that considering that this is my third time here in three months, most of the audience knows who I am.  But, for those of you who have not met me before; my name is Dawn; I do cloud security and DevOps type work at Innablr - and we are hiring, so I'm sure you'll be hearing from me again in the marketplace segment; erm, what probably pretty much no one here knows about me though, is that I am the child of two PhD holders.  It's probably not immediately apparent what being the child of two PhD holders has to do with defining infrastructure as code across your organisation.  Erm, for that I would say to you all that being the child of two PhD holders, and having read their PhDs, teaches you the value in repeatable experiments, and in being able to validate your findings in multiple scenarios.  Outside of work, I am an occasional author and kitchen alchemist - sometimes that ends really well, sometimes it doesn't.  I'm also a raging sportsball fan, and I am taking a break from the hockey garb which has been common recently, and we're going with New England Patriots football gear instead today.

PATRIOTS FAN IN AUDIENCE: [claps]

DAWN: So, back to that thing about being the child of two PhDs.  The reason why it is relevant for this talk is because, the- although what I'm talking about is infrastructure as code tools, this is predicated on a concept called Conway's law, which, as the name would suggest, was invented by a gentleman called Conway.  A paraphrase of what this is about is to say that 'Conway's law is- the maxim that an organisation will always design - tools, software, products, whatever - which reflect the organisation's structure'.  And that means that by first principles, if you want to design yourself a system, you have to start by designing the structures that are going to build it.  You have to design the people which are going to build it.  You have to design an architecture of teams; an architecture of reporting which will enable you to actually build what you want to build.  And as befits my parents and their PhDs, Conway's law has actually been validated in multiple academic settings by researchers who have gone and looked at big companies such as Microsoft.  And they found that generally this maxim holds true.

So why is it that org structure matters? This is signifi- or, my take on this, at least, is significantly less researched than the fact that it does, but the first thing that I would call out here is that if teams actually talk to each other, they're going to work much faster, and the work that they're going to produce is generally going to be a lot better.  When you have people sitting alone in silos developing things, there's often a lot of information in people's heads which is not cross-pollinating.  And if that information were cross-pollinating, it would potentially speed up the work that you were trying to do.  A colleague of mine at Innablr called Colin Liddle - really great guy - wrote a blog post for us a while back talking about the idea of platform teams.  Colin and I also worked together at a previous job, and both of us were, at various times, part of the platform team there.  And one takeaway that he called out in that blog post which I absolutely agree with is that the paradigm of the platform team, which is one which is- sort of becoming much more common now - it means that if you have people who are maintaining your baseline, you free up the dev teams to get work done.

If every team is managing their own Kubernetes cluster, and I know that Kubernetes is becoming a bit of a schtick here, but that means that 95% of their time is going to be taken up actually managing the infra to make sure that it's working.  Even if it is Kubernetes, if you can centralise that all in one place and have a group of specialists managing it, then all that the people who are deploying to it need to know is about what they need to do to deploy.  And those dedicated support people are inherently going to reduce blockers.  They are going to reduce lead times.  Because when you have a problem that's not in your domain, if you've got dedicated support people who can help you with it, you're not smashing your head repeatedly against a brick wall.  You can go to them, interact with them, and that will enable you to get things done quicker.  And ultimately this idea of, sort of, cross-functional working groups ends up being a bit fallacious a lot of the time, because - they're not consistent, and they're often arranged on an ad-hoc basis.  If you have a simple and appropriate team structure, which kind of matches the size and the scale of your organisation, in the end, what you're going to end up with is a simpler final product.  For large organisations, some degree of complexity is inherent to their structure, but if you can work out how to harness that complexity in the correct way, it will allow you to produce software which is ultimately much more simple.

So, this brings me to the question of how to architect a company - but, as those of you who've seen all of my talks would know, it's probably better described as how not to architect a company.  Because a lot of the companies that are around - even big companies, even companies like AWS - once upon a time were a bunch of people in a shed, or a bunch of people working for another company, who eventually got spun off into their own structure.  And if you do not think about solution architecture and design architecture for your organisation, what you end up doing is building complexity in on the fly.  You scale, but you don't necessarily scale in a way that's going to keep you running at the same velocity when you're ten times or a hundred times bigger.  And what a lot of that complexity tends to come in the form of i- are siloed teams that don't have a central support structure.  You need to scale, you spin up another team to run it.  But then you've got that problem of a whole bunch of redundant work is being done.  There's information in multiple people's heads.  Everyone has various different pieces of the jigsaw, but you can't put together the puzzle until you get all of them together, so they have to go and find the bits themselves.

The other fallacy, which I think is slightly less common but I'm aware of a few examples of this in the wild, is when you get really deep team-based hierarchies.  So rather than one or two levels, you've got four or five.  You've got different levels of teams that are building complexity on top of each other.  And what will tend to happen there is you lose critical information in communication.  If you've got one people that are buildi- one group of people that are building the infrastructure, and then you've got another group of people that are building on top of that, and a third group that's building on top of that, and then a fourth group that's finally building your application, how does the information that the application team need from the people who are building that core infrastructure get through those layers of hierarchy?

And again, as befits my talks at the AWS User Group, once again, we are going to return to my favourite case study.  And that is our friends at ProductCorp and their newly acquired subsidiary Fly Money.  I'm not going to do the full introduction, because I'm sure that most of you have met ProductCorp and Fly Money by now, but we're talking here about two startups.  ProductCorp builds primarily e-commerce software, and they have two flagship products; BuyIt and SellIt.  They run everything on AWS.  And the reason why they bought Fly Money is because they were paying a significant amount of money to a payments processor to handle all of the payments on their purchasing software, and they decided that they needed to cut out the middleman.  And, we have- anyone who has seen my ProductCorp talks before will also have met the wonderful Ben Nguyen.  They are ProductCorp's Head of Engineering, and they have over 10 years of experience, mainly in the venue of startups.  To be honest, given how long I've been giving these talks, it's probably up near 15 now.  But one thing that's really important to understand about Ben and the way that they approach things is that they're very passionate about improving infrastructure.  And they're very well aware that part of what comes out of that is - it's not just about improving infrastructure, you have to improve the structures that people are working in to enable them to build better things.

Which leads us to- taking a few steps back, as we've sort of seen this acquisition unfold.  Let me tell you a story about the first meeting of minds.  This is when Ben got all of the employees of Pr- of Fly Money's engineering department into a room, and they had a clash with meetings.  So they got everyone in the room and they said 'What I want you to do, while I'm gone for these 15 minutes to take this critical call, is sit down and draw me out an org chart.  You know, I've heard about how your company operates, but what I really want to understand is from your point of view, what do you see as the structure of the organisation?  So get together, draw that out, and I'll be back in 15 minutes'. Ben came back 15 minutes later to not one org chart but one orc chart per person.  And the real kicker here, which shows you just how dysfunctional Fly Money is, is that there were absolutely no similarities between any of these 20-some org charts.  In fact, the one that turned out to be the most accurate was the one which just had the employee's name on it, and then drew out a chart with a bunch of question marks all over the place.  So as we can see from this, this is not really a company that has any sort of functioning structure.

And this brings me to the question of what Fly Money's initial infrastructure actually looked like.  Because the art of defining repeatable infrastructure is something that's - I mean, it's a fine art.  If you do it well it helps you to increase your velocity, but it's actually really, really difficult to do.  So the question there becomes who's actually responsible for things?  And this is my, ah, entirely anecdotal opinion on one particular model of responsibility that I have seen work very well when you're dealing with infrastructure as code.  So what you essentially have here is you have two groups of people.  You have- people who are in central teams, which are responshibl- responsible for each core component of the platform.  And that might be people who are responsible for AWS.  It might be people who are responsible for your ECS Fargate.  It might be people who are responsible for your Kubernetes, or, you know, maintaining things like your central security, but that might also be people who deal with other vendors.  People who are responsible for for provisioning for things like your Github or your Gitlab.  People who are responsible for provisioning other tools which are very important for you to be able to use, and keeping them up to date.  And then you have the dev teams, who are actually building the software on top of this.  They've got the core platform.  They don't need to be able to necessarily see under the hood of how all of that is working, all that they care about is that it works and that they have someone that they can go to to talk about it if they're running into any issues.

So your central teams are responsible for building and architecting those core components.  Getting everyone in a room, deciding how it should run, and then setting it up.  They're also responsible for providing some sort of pattern that dev teams can consume to allow them to use the software.  And if you're talking about a Git provider that's probably going to be very simple, but if you're talking about some sort of platform built on AWS, whether that be just a basic account structure or whether it be that you're actually providing a lot of the building blocks to people, it's going to need to be significantly more complex.  You can't rely on dev teams being able to understand CloudFormation defined in YAML.  And finally, they're responsible for meeting internal SLAs.  So they are responsible for being able to go to the dev teams and say 'we expect this to be up this amount of time, architect your software accordingly, and then this will inform the SLAs that you give to other businesses.  When you're talking to sales people, when you're talking to marketing people, these are the baselines you're working from, and then you build in your own slack'.

And the dev teams are responsible for taking those core patterns and customising them to their use-case.  It might be that they're basically usable out of the box, or it might be that there's a significant amount of customisation that needs to happen on top.  And they are also responsible for automated and reliable testing and deployment of their applications, and this is absolutely critical.  Although the core infra is only as good as the infrastructure as code and the processes that define it, the same is true for the application.  An application is only as good as the automation and reliability of the testing and deployment process.  You do not want to be picking up your major issues in manual UAT testing - even though manual UAT testing is still something that you should be doing.

And then we have a bunch of different tools which we can leverage to do all of this.  Infrastructure provisioning tools, which are the ones that we would generally actually use to set up stuff on AWS, fall into two buckets: you've got ones which are declarative, where you just write down what you want and it spits it out; and you've got ones which are programmatic, where you can set up functionality, feed it some information and it will generate it for you.  So we're obviously in AWS land right now, your main declarative tool is going to be CloudFormation.  If, for some reason, you're using Azure day to day and you're here at an AWS meet up - well, first off, kudos to you, but what you're looking for there are ARM templates.  And if, as is more likely, you're working in a multi-cloud environment or a hybrid cloud environment, the tools which are open-source to at least a degree which you can use in any scenario are going to be your Pulumis and your Terraforms.  There's always utility in being able to understand how these work, but it's also very important if you're working somewhere like a big bank or an emergency services provider where they need to actually run things on multiple clouds to provide a level of redundancy.  Whole bunch of fallacies around those sorts of things being multi-cloud or hybrid cloud - they're not, you cannot take the configuration from one of- one cloud and immediately plug it into another, but if you understand how to write Terraform for AWS, you understand how to write Terraform for Azure or GCP.  And programmatically, you've got our very own AWS CDK, which HashiCorp have built on to provide the CDK for Terraform.  These basically take really common programming languages: you define constructs, you tell them what you want, and they will generate the declarative template for you.  This is really, really powerful, although it comes with one or two major downsides.

The other big bucket that you've got here, which are the ones that are sort of falling out of favour, are the config management tools.  And there are four and a half of these that are very commonly used.  The four are Ansible, Chef, Puppet, and Salt.  And what these allow you to do is to basically define a list of changes that you want to make to your server, and then the config management tool will go away and make them.  The reason why these are slowly becoming obsolete is because - the most repeatable way to do config management is to actually build it into a container image so that you don't have to go away and manually do it on each server.

So how does this all apply to Fly Money?  Well, those of you who heard the disaster recovery presentation two months ago would know that they had some of their infrastructure as code partially defined in Terraform, but they've also got a whole bunch of it defined in CLI scripts.  So what you've essentially got there is - very similar to the config management tool - an imperative list of instructions that go 'hey, go away on my server and do this', rather than a declarative template that says 'for this particular structure, I want you to get it to this end state'.  What's worse is that because Fly Money's org chart is dysfunctional at best and non-existent at worst, about 60% of the work that's being done there is repeatable.  You don't need to invent the wheel on how to deploy an EC2 instance five times.  If you can find one pattern that works for deploying one, that will get you to where you want to be.  And their config management, contrary to the current common paradigm, is all done with Ansible and Chef.  But the real kicker here is the bulk of their resources, so about 55%, are not actually defined anywhere at all.  Someone has just gone in on the console, created these, and not thought to make them automatable.

